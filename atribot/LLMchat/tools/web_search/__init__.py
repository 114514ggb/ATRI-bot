import aiohttp
from typing import List, Dict, Optional, Any
from atribot.core.service_container import container


headers = {
    "Authorization": f"Bearer {container.get("config").model.tavily_search_API_key}",
    "Content-Type": "application/json"
}

tool_json = {
    "name": "web_search",
    "description": "å¼ºå¤§çš„ç½‘ç»œæœç´¢å·¥å…·ï¼Œæä¾›å…¨é¢ã€å®æ—¶çš„æœç´¢ç»“æœã€‚æ”¯æŒè‡ªå®šä¹‰ç»“æœæ•°é‡ã€å†…å®¹ç±»å‹ä¸åŸŸåç­›é€‰ï¼Œæ˜¯è·å–å®æ—¶ä¿¡æ¯ã€æ–°é—»ä¸è¿›è¡Œç½‘ç»œå†…å®¹åˆ†æçš„ç†æƒ³æ–¹æ¡ˆ",
    "properties": {
        "query": {
            "type": "string",
            "description": "Search query"
        },
        "search_depth": {
            "type": "string",
            "enum": [
                "basic",
                "advanced"
            ],
            "description": "The depth of the search. It can be 'basic' or 'advanced'",
            "default": "advanced"
        },
        "topic": {
            "type": "string",
            "enum": [
                "general",
                "news"
            ],
            "description": "The category of the search. This will determine which of our agents will be used for the search",
            "default": "general"
        },
        "days": {
            "type": "number",
            "description": "The number of days back from the current date to include in the search results. This specifies the time frame of data to be retrieved. Please note that this feature is only available when using the 'news' search topic",
            "default": 3
        },
        "time_range": {
            "type": "string",
            "description": "The time range back from the current date to include in the search results. This feature is available for both 'general' and 'news' search topics",
            "enum": [
                "day",
                "week",
                "month",
                "year",
                "d",
                "w",
                "m",
                "y"
            ]
        },
        "max_results": {
            "type": "number",
            "description": "The maximum number of search results to return",
            "default": 10,
            "minimum": 5,
            "maximum": 20
        },
        "include_images": {
            "type": "boolean",
            "description": "Include a list of query-related images in the response",
            "default": False
        },
        "include_image_descriptions": {
            "type": "boolean",
            "description": "Include a list of query-related images and their descriptions in the response",
            "default": False
        },
        "include_raw_content": {
            "type": "boolean",
            "description": "Include the cleaned and parsed HTML content of each search result",
            "default": False
        },
        "include_domains": {
            "type": "array",
            "items": {
                "type": "string"
            },
            "description": "A list of domains to specifically include in the search results, if the user asks to search on specific sites set this to the domain of the site",
            "default": []
        },
        "exclude_domains": {
            "type": "array",
            "items": {
                "type": "string"
            },
            "description": "List of domains to specifically exclude, if the user asks to exclude a domain set this to the domain of the site",
            "default": []
        }
    }
}



async def main(
        query: str,
        search_depth: str = "basic",
        topic: str = "general",
        days: int = 3,
        time_range: Optional[str] = None,
        max_results: int = 10,
        include_images: bool = False,
        include_image_descriptions: bool = False,
        include_raw_content: bool = False,
        include_domains: Optional[List[str]] = None,
        exclude_domains: Optional[List[str]] = None
    ):
    return await web_search(
        query=query, 
        search_depth=search_depth, 
        topic=topic,
        days=days,
        time_range=time_range,
        max_results=max_results,
        include_images=include_images,
        include_image_descriptions=include_image_descriptions,
        include_raw_content=include_raw_content,
        include_domains=include_domains,
        exclude_domains=exclude_domains
    )


async def web_search(
    query: str,
    auto_parameters:bool = False,
    search_depth: str = "basic",
    topic: str = "general",
    days: int = 3,
    time_range: Optional[str] = None,
    max_results: int = 10,
    include_answer: Optional[str] = None,
    include_images: bool = False,
    include_image_descriptions: bool = False,
    include_raw_content: bool = False,
    include_domains: Optional[List[str]] = None,
    exclude_domains: Optional[List[str]] = None
):
    """
    Tavily Search api
    https://docs.tavily.com/documentation/api-reference/endpoint/search
    è¯¥å‡½æ•°å°è£…äº† Tavily Search APIï¼Œæ”¯æŒé€šç”¨æœç´¢å’Œæ–°é—»æœç´¢ï¼Œå…è®¸è‡ªå®šä¹‰æœç´¢æ·±åº¦ã€
    æ—¶é—´èŒƒå›´ã€ç»“æœæ•°é‡ä»¥åŠå†…å®¹è¿‡æ»¤ï¼ˆå¦‚åŒ…å«/æ’é™¤ç‰¹å®šåŸŸåï¼‰ã€‚

    Args:
        query (str): 
            æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²ï¼ˆä¾‹å¦‚ï¼š"Who is Leo Messi?"ï¼‰ã€‚
        
        auto_parameters (bool):
            è‡ªåŠ¨æ ¹æ®æŸ¥è¯¢å†…å®¹å’Œæ„å›¾é…ç½®æœç´¢å‚æ•°ã€‚ä½ ä»ç„¶å¯ä»¥æ‰‹åŠ¨è®¾ç½®å…¶ä»–å‚æ•°ï¼Œä½ çš„æ˜¾å¼æ•°å€¼ä¼šè¦†ç›–è‡ªåŠ¨å‚æ•°
            include_raw_content å’Œ max_results å¿…é¡»å§‹ç»ˆæ‰‹åŠ¨è®¾ç½®ï¼Œå› ä¸ºå®ƒä»¬ç›´æ¥å½±å“å“åº”å¤§å°
        
        search_depth (str, optional): 
            æœç´¢æ·±åº¦ã€‚é»˜è®¤ä¸º "basic"ã€‚
            - "basic": å¿«é€Ÿæœç´¢ï¼Œè¿”å›åŸºç¡€ç»“æœã€‚
            - "advanced": æ·±å…¥æœç´¢ï¼Œç»“æœæ›´ç›¸å…³ã€è´¨é‡æ›´é«˜ï¼Œä½†è€—æ—¶ç¨é•¿ï¼ˆæ¶ˆè€— 2 creditsï¼‰ã€‚
        
        topic (str, optional): 
            æœç´¢ä¸»é¢˜ç±»åˆ«ã€‚é»˜è®¤ä¸º "general"ã€‚
            - "general": é€šç”¨æœç´¢ï¼Œè¦†ç›–å¹¿æ³›æ¥æºã€‚
            - "news": æ–°é—»æœç´¢ï¼Œä¾§é‡äºå®æ—¶æ›´æ–°å’Œä¸»æµåª’ä½“ã€‚
            
        days (int, optional): 
            å›æº¯å¤©æ•°ã€‚é»˜è®¤ä¸º 3ã€‚
            ä»…å½“ topic ä¸º "news" æ—¶ç”Ÿæ•ˆï¼ŒæŒ‡å®šæœç´¢è¿‡å»å¤šå°‘å¤©å†…çš„æ–°é—»æ•°æ®ã€‚
        
        time_range (Optional[str], optional): 
            æ—¶é—´èŒƒå›´è¿‡æ»¤å™¨ã€‚é»˜è®¤ä¸º Noneã€‚
            ç”¨äºç­›é€‰å‘å¸ƒæ—¶é—´æˆ–æ›´æ–°æ—¶é—´ã€‚å¯é€‰å€¼åŒ…æ‹¬ï¼š
            "day", "week", "month", "year", "d", "w", "m", "y"ã€‚
            é€‚ç”¨äº "general" å’Œ "news" ä¸»é¢˜ã€‚
        
        max_results (int, optional): 
            è¿”å›çš„æœ€å¤§æœç´¢ç»“æœæ•°é‡ã€‚é»˜è®¤ä¸º 10ã€‚
            æœ‰æ•ˆèŒƒå›´é€šå¸¸ä¸º 5 åˆ° 20ã€‚
        
        include_answer (str)
            æ˜¯å¦åŒ…å«å¯¹ç”¨æˆ·æŸ¥è¯¢çš„ç®€çŸ­å›ç­”ï¼Œç”±å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆã€‚å¯è®¾ç½®ä¸ºFalse,true,basic æˆ– advanced
    
        include_images (bool, optional): 
            æ˜¯å¦åœ¨å“åº”ä¸­åŒ…å«ä¸æŸ¥è¯¢ç›¸å…³çš„å›¾ç‰‡åˆ—è¡¨ã€‚é»˜è®¤ä¸º Falseã€‚
        
        include_image_descriptions (bool, optional): 
            æ˜¯å¦åŒ…å«å›¾ç‰‡çš„æè¿°æ–‡æœ¬ã€‚é»˜è®¤ä¸º Falseã€‚
            ä»…å½“ include_images ä¸º True æ—¶ç”Ÿæ•ˆã€‚
        
        include_raw_content (bool, optional): 
            æ˜¯å¦åŒ…å«æ¯ä¸ªæœç´¢ç»“æœçš„æ¸…æ´—å HTMLåŸå§‹å†…å®¹ã€‚é»˜è®¤ä¸º Falseã€‚
            å¦‚æœä¸º Trueï¼Œå“åº”ä½“å°†å˜å¤§ã€‚
        
        include_domains (Optional[List[str]], optional): 
            æŒ‡å®šåŒ…å«çš„åŸŸååˆ—è¡¨ï¼ˆç™½åå•ï¼‰ã€‚é»˜è®¤ä¸º Noneã€‚
            å¦‚æœè®¾ç½®ï¼Œæœç´¢ç»“æœå°†ä»…é™äºè¿™äº›åŸŸåï¼ˆä¾‹å¦‚ï¼š["wsj.com", "wikipedia.org"]ï¼‰ã€‚
        
        exclude_domains (Optional[List[str]], optional): 
            æŒ‡å®šæ’é™¤çš„åŸŸååˆ—è¡¨ï¼ˆé»‘åå•ï¼‰ã€‚é»˜è®¤ä¸º Noneã€‚
            å¦‚æœè®¾ç½®ï¼Œæœç´¢ç»“æœå°†è‡ªåŠ¨è¿‡æ»¤æ‰è¿™äº›åŸŸåã€‚

    Returns:
        str:äººç±»å¯è¯»çš„str
    """

    if include_domains is None:
        include_domains = []
    if exclude_domains is None:
        exclude_domains = []

    if auto_parameters:
        payload = {
            "query": query,
            "max_results": max_results,
            "include_raw_content": include_raw_content,
            "auto_parameters":True,
        }
    else:
        payload = {
            "query": query,
            "topic": topic,
            "search_depth": search_depth,
            "max_results": max_results,
            "days": days,
            "include_images": include_images,
            "include_image_descriptions": include_image_descriptions,
            "include_raw_content": include_raw_content,
            "include_domains": include_domains,
            "exclude_domains": exclude_domains,
            "include_answer": False
        }

    if time_range:
        payload["time_range"] = time_range

    if include_answer:
        payload["include_answer"] = include_answer

    async with aiohttp.ClientSession() as session:
        async with session.post(
            url="https://api.tavily.com/search",
            headers=headers,
            json=payload
        ) as resp:
            json = await resp.json()
            print(json)
            return format_search_results(json)
        


def format_search_results(response_data: Dict[str, Any]) -> str:
    """
    å°† Tavily æœç´¢ç»“æœ JSON è½¬æ¢ä¸º LLM æ˜“è¯»çš„ Markdown æ–‡æœ¬æ ¼å¼ã€‚
    
    Args:
        response_data (dict): Tavily API è¿”å›çš„åŸå§‹ JSON æ•°æ®ã€‚
        
    Returns:
        str: æ ¼å¼åŒ–åçš„ Markdown å­—ç¬¦ä¸²ã€‚
    """
    lines = []

    lines.append(f"# ğŸ” æœç´¢æŸ¥è¯¢: {response_data.get("query", "æœªçŸ¥æŸ¥è¯¢")}\n")

    if answer :=  response_data.get("answer"):
        lines.append("## ğŸ’¡ æ™ºèƒ½æ‘˜è¦")
        lines.append(f"{answer}\n")

    if results := response_data.get("results", []):
        lines.append(f"## ğŸ“„ æœç´¢ç»“æœæ¥æº ({len(results)}æ¡)")
        
        for idx, item in enumerate(results, 1):
            lines.append(f"### æ¥æº {idx}: [{item.get("title", "æ— æ ‡é¢˜")}]({item.get("url", "#")})")
                         
            if content := item.get("content", "æ— å†…å®¹æ‘˜è¦"):
                lines.append(f"> **æ‘˜è¦**: {content}")
            
            # å¦‚æœæœ‰ raw_content é€šå¸¸å¤ªé•¿
            if raw_content := item.get("raw_content"):
                lines.append(f"æˆªæ–­çš„åŸå§‹html:{raw_content[:1000]}")
    else:
        lines.append("## ğŸ“„ æœç´¢ç»“æœ: æœªæ‰¾åˆ°ç›¸å…³é¡µé¢\n")

    if images:= response_data.get("images", []):
        lines.append(f"## ğŸ–¼ï¸ ç›¸å…³å›¾ç‰‡ ({len(images)}å¼ )")
        for img in images:
            if isinstance(img, str):
                lines.append(f"- ![]({img})")
            elif isinstance(img, dict):
                url = img.get("url", "")
                desc = img.get("description", "å›¾ç‰‡")
                lines.append(f"- [{desc}]({url})")
        lines.append("")

    if response_time:= response_data.get("response_time"):
        lines.append(f"*æœç´¢è€—æ—¶: {response_time}ç§’*")

    return "\n".join(lines)
    